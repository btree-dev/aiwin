{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune a distilBERT Model for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './output/model/'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 8\n",
    "SHUFFLE = True\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 872/872 [00:00<00:00, 16830.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Set up Tokenizer and Model\n",
    "\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\") # Python implementation, slower\n",
    "# Rust implementation, faster\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Tokenize and preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# collate function to handle padding\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    attention_masks = [torch.tensor(item['attention_mask']) for item in batch]\n",
    "    labels = [torch.tensor(item['label']) for item in batch]\n",
    "    \n",
    "    # Pad sequences to the same length\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)  # Assuming 0 as the padding value for attention masks\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "# Prepare DataLoader with collate function\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=SHUFFLE, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.11007733643054962\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning setup\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        inputs = {key: batch[key] for key in [\"input_ids\", \"attention_mask\", \"labels\"]}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Print training statistics if needed\n",
    "    print(f\"Epoch {epoch + 1}/{N_EPOCHS}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save finetuned model to file\n",
    "torch.save(model.state_dict(), f\"{output_path}/distilbert-sst2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHA-256 Hash: dd2a7a30f0822b2d5d6a4b1ead1ceafea2cc15867fc3650b7f5348e64f3818e0\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def sha256_hash_file(file_path):\n",
    "    # Create a SHA-256 hash object\n",
    "    sha256_hash = hashlib.sha256()\n",
    "\n",
    "    # Open the file in binary mode and read it in chunks\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Read the file in chunks of 4096 bytes (4 KB)\n",
    "        for byte_block in iter(lambda: file.read(4096), b''):\n",
    "            sha256_hash.update(byte_block)\n",
    "\n",
    "    # Get the hexadecimal representation of the hash\n",
    "    hash_result = sha256_hash.hexdigest()\n",
    "\n",
    "    return hash_result\n",
    "\n",
    "# Hash the entire model\n",
    "hashed_result = sha256_hash_file(\"../pytorch_model.bin\")\n",
    "\n",
    "print(\"SHA-256 Hash:\", hashed_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_merkle_tree_root(uid_list):\n",
    "    # Convert UUIDs to bytes and hash each leaf node\n",
    "    hashed_leaves = [hashlib.sha256(uid.bytes).hexdigest() for uid in uid_list]\n",
    "\n",
    "    # If the number of leaves is odd, duplicate the last leaf to make it even\n",
    "    if len(hashed_leaves) % 2 != 0:\n",
    "        hashed_leaves.append(hashed_leaves[-1])\n",
    "\n",
    "    # Build the Merkle tree by iteratively hashing pairs of nodes\n",
    "    while len(hashed_leaves) > 1:\n",
    "        hashed_leaves = [hashlib.sha256(hashlib.sha256(hashed_leaves[i].encode() + hashed_leaves[i + 1].encode()).digest()).hexdigest()\n",
    "                         for i in range(0, len(hashed_leaves), 2)]\n",
    "\n",
    "    # The last remaining element is the Merkle tree root\n",
    "    merkle_tree_root = hashed_leaves[0]\n",
    "\n",
    "    return merkle_tree_root\n",
    "\n",
    "# # Example usage with a list of UUIDs\n",
    "# uuid_list = [uuid.uuid4() for _ in range(4)]\n",
    "# merkle_tree_root = compute_merkle_tree_root(uuid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create a dictionary with your variables\n",
    "data = {\n",
    "    \"model_name\": \"distilBERT-sst-2-sentiment-classifier\",\n",
    "    \"random_seed\": 42,\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"batch_size\" : 8,\n",
    "    \"shuffle\": True,\n",
    "    \"model_weights_hash(sha256)\": hashed_result\n",
    "}\n",
    "\n",
    "json_file_path = \"./output/sample_model_card.json\"\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
